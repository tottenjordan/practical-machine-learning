{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jt_tf_experiement.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "U06NbY7WnJHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **This notebook compares different tensorflow neural netowrk designs using the MNIST dataset**"
      ]
    },
    {
      "metadata": {
        "id": "oDCYci10hWbP",
        "colab_type": "code",
        "outputId": "2598d373-1fa3-4c67-d38c-0cbaaa141a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w_DnjaZpipns",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from functools import wraps\n",
        "from time import time\n",
        "def timing(f):\n",
        "    @wraps(f)\n",
        "    def wrap(*args, **kw):\n",
        "        ts = time()\n",
        "        result = f(*args, **kw)\n",
        "        te = time()\n",
        "        print(f\"fun: {f.__name__}, args: [{args}, {kw}] took: {te-ts} sec\")\n",
        "        return result\n",
        "    return wrap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUyXAN7LjEHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Global Parameters**\n",
        "\n",
        "Consistent for every model"
      ]
    },
    {
      "metadata": {
        "id": "YONIG_xJir7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Global Parameters\n",
        "batch_size = 100\n",
        "display_step = 9\n",
        "\n",
        "# Network Parameters\n",
        "n_input = 784 # MNIST data input (28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "x = tf.placeholder(\"float\", [None, n_input])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N04Zbl60kWpc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Multilayer Perceptron Function**"
      ]
    },
    {
      "metadata": {
        "id": "Xh8vqJneko-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "  \n",
        "    # Use tf.matmul (broadcast)\n",
        "    print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())  \n",
        "    \n",
        "    # Hidden layer with RELU activation\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "\n",
        "    # Hidden layer with RELU activation\n",
        "    print( 'layer_1:', layer_1.get_shape(), 'W2:', weights['h2'].get_shape(), 'b2:', biases['b2'].get_shape())        \n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "\n",
        "    # Output layer with linear activation\n",
        "    print( 'layer_2:', layer_2.get_shape(), 'W3:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
        "    print('out_layer:',out_layer.get_shape())\n",
        "\n",
        "    return out_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pb05KyqVjNMA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model A**\n",
        "\n",
        "*Try nodes equal to the square root of the input features*"
      ]
    },
    {
      "metadata": {
        "id": "lSMxuHlGivRL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model A learning Rate\n",
        "learning_rate_A = 0.001\n",
        "training_epochs_A = 100\n",
        "\n",
        "# Model A Network Parameters\n",
        "A_hidden_1 = 28 # 1st layer number of features\n",
        "A_hidden_2 = 28 # 2nd layer number of features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTmhCZWRjvxd",
        "colab_type": "code",
        "outputId": "8bf86938-038b-4747-8c73-addd5dee6f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model B\n",
        "weights_A = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, A_hidden_1])),    #784x28\n",
        "    'h2': tf.Variable(tf.random_normal([A_hidden_1, A_hidden_2])), #28x28\n",
        "    'out': tf.Variable(tf.random_normal([A_hidden_2, n_classes]))  #28x10\n",
        "}\n",
        "biases_A = {\n",
        "    'b1': tf.Variable(tf.random_normal([A_hidden_1])),             #28x1\n",
        "    'b2': tf.Variable(tf.random_normal([A_hidden_2])),             #28x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model B\n",
        "pred_A = multilayer_perceptron(x, weights_A, biases_A)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 28) b1: (28,)\n",
            "layer_1: (?, 28) W2: (28, 28) b2: (28,)\n",
            "layer_2: (?, 28) W3: (28, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B0YO_hHclaoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model A\n",
        "cost_A = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_A, labels=y))\n",
        "\n",
        "# Optimizer for Model A\n",
        "optimizer_A = tf.train.AdamOptimizer(learning_rate=learning_rate_A).minimize(cost_A)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "00Vjoy5ZloTb",
        "colab_type": "code",
        "outputId": "6d0fed5c-2370-46ca-df54-034bdf40505f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Model A\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_A():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_A):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_A, cost_A], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model A Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_A, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model A:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_A()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 33.519423692\n",
            "Epoch: 0010 cost= 0.896983690\n",
            "Epoch: 0019 cost= 0.364546590\n",
            "Epoch: 0028 cost= 0.227477948\n",
            "Epoch: 0037 cost= 0.170262680\n",
            "Epoch: 0046 cost= 0.142203039\n",
            "Epoch: 0055 cost= 0.116627858\n",
            "Epoch: 0064 cost= 0.104153705\n",
            "Epoch: 0073 cost= 0.092194962\n",
            "Epoch: 0082 cost= 0.079351897\n",
            "Epoch: 0091 cost= 0.071888410\n",
            "Epoch: 0100 cost= 0.065250891\n",
            "Model A Optimization Finished!\n",
            "Accuracy of Model A: 0.9497\n",
            "fun: training_loop_A, args: [(), {}] took: 136.6531891822815 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yyG_Hy5wmUof",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model B**\n",
        "\n",
        "*Same nodes and epochs as Model A, adjust learning rate*"
      ]
    },
    {
      "metadata": {
        "id": "t63nA8qemYqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model B learning Rate\n",
        "learning_rate_B = 0.1\n",
        "training_epochs_B = 100\n",
        "\n",
        "# Model B Network Parameters\n",
        "B_hidden_1 = 28 # 1st layer number of features\n",
        "B_hidden_2 = 28 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OEor48mmfAT",
        "colab_type": "code",
        "outputId": "1bab1f78-ac62-4292-99ec-1253ec693146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model B\n",
        "weights_B = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, B_hidden_1])),    #784x28\n",
        "    'h2': tf.Variable(tf.random_normal([B_hidden_1, B_hidden_2])), #28x28\n",
        "    'out': tf.Variable(tf.random_normal([B_hidden_2, n_classes]))  #28x10\n",
        "}\n",
        "biases_B = {\n",
        "    'b1': tf.Variable(tf.random_normal([B_hidden_1])),             #28x1\n",
        "    'b2': tf.Variable(tf.random_normal([B_hidden_2])),             #28x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model B\n",
        "pred_B = multilayer_perceptron(x, weights_B, biases_B)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 28) b1: (28,)\n",
            "layer_1: (?, 28) W2: (28, 28) b2: (28,)\n",
            "layer_2: (?, 28) W3: (28, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYdDqxOKmuJf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model B\n",
        "cost_B = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_B, labels=y))\n",
        "\n",
        "# Model B Optimizer\n",
        "optimizer_B = tf.train.AdamOptimizer(learning_rate=learning_rate_B).minimize(cost_B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6uIp0oX3nJGa",
        "colab_type": "code",
        "outputId": "9c7d8909-c361-4dc3-bdea-12cf3b3b090e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Model B\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_B():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_B):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_B, cost_B], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model B Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_B, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model B:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_B()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 2.910077302\n",
            "Epoch: 0010 cost= 1.733886617\n",
            "Epoch: 0019 cost= 1.915577514\n",
            "Epoch: 0028 cost= 1.860436428\n",
            "Epoch: 0037 cost= 1.880057339\n",
            "Epoch: 0046 cost= 1.845974970\n",
            "Epoch: 0055 cost= 1.857720120\n",
            "Epoch: 0064 cost= 1.860988697\n",
            "Epoch: 0073 cost= 1.894884940\n",
            "Epoch: 0082 cost= 1.894137257\n",
            "Epoch: 0091 cost= 1.840635065\n",
            "Epoch: 0100 cost= 1.834093482\n",
            "Model B Optimization Finished!\n",
            "Accuracy of Model B: 0.2094\n",
            "fun: training_loop_B, args: [(), {}] took: 137.72603511810303 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "42cXEwX0oTik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model C**\n",
        "\n",
        "*increase hidden layer nodes to 300*"
      ]
    },
    {
      "metadata": {
        "id": "Jw9ZXvlhop6P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model C learning Rate\n",
        "learning_rate_C = 0.001\n",
        "training_epochs_C = 100\n",
        "\n",
        "# Model C Network Parameters\n",
        "C_hidden_1 = 300 # 1st layer number of features\n",
        "C_hidden_2 = 300 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gt9RnFD_oyhm",
        "colab_type": "code",
        "outputId": "ee0f36ba-c783-4ddb-c3a5-fd7f53a18410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model C\n",
        "weights_C = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, C_hidden_1])),    #784x300\n",
        "    'h2': tf.Variable(tf.random_normal([C_hidden_1, C_hidden_2])), #300x300\n",
        "    'out': tf.Variable(tf.random_normal([C_hidden_2, n_classes]))  #300x10\n",
        "}\n",
        "biases_C = {\n",
        "    'b1': tf.Variable(tf.random_normal([C_hidden_1])),             #300x1\n",
        "    'b2': tf.Variable(tf.random_normal([C_hidden_2])),             #300x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model C\n",
        "pred_C = multilayer_perceptron(x, weights_C, biases_C)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 300) b1: (300,)\n",
            "layer_1: (?, 300) W2: (300, 300) b2: (300,)\n",
            "layer_2: (?, 300) W3: (300, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k-GxNeXgpEZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model C\n",
        "cost_C = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_C, labels=y))\n",
        "\n",
        "# Model C Optimizer\n",
        "optimizer_C = tf.train.AdamOptimizer(learning_rate=learning_rate_C).minimize(cost_C)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iVNDpCxxpTo_",
        "colab_type": "code",
        "outputId": "11df32fe-3074-4697-d911-10b4287cccf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Model C\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_C():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_C):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_C, cost_C], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model C Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_C, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model C:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_C()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 177.714622806\n",
            "Epoch: 0010 cost= 2.648205742\n",
            "Epoch: 0019 cost= 0.482994352\n",
            "Epoch: 0028 cost= 0.406804922\n",
            "Epoch: 0037 cost= 0.212329539\n",
            "Epoch: 0046 cost= 0.204479781\n",
            "Epoch: 0055 cost= 0.210383427\n",
            "Epoch: 0064 cost= 0.142848322\n",
            "Epoch: 0073 cost= 0.138408895\n",
            "Epoch: 0082 cost= 0.124967998\n",
            "Epoch: 0091 cost= 0.117228675\n",
            "Epoch: 0100 cost= 0.102824890\n",
            "Model C Optimization Finished!\n",
            "Accuracy of Model C: 0.9682\n",
            "fun: training_loop_C, args: [(), {}] took: 155.43393301963806 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MegwqYnWqW1w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model D**\n",
        "\n",
        "*Same nodes as Model C, adjust learning rate*"
      ]
    },
    {
      "metadata": {
        "id": "fE5olUFsqiGb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model D learning Rate\n",
        "learning_rate_D = 0.01\n",
        "training_epochs_D = 100\n",
        "\n",
        "# Model D Network Parameters\n",
        "D_hidden_1 = 300 # 1st layer number of features\n",
        "D_hidden_2 = 300 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zcggFKjoqplL",
        "colab_type": "code",
        "outputId": "7ecc1409-ce49-4d48-8525-0db1452a593f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model D\n",
        "weights_D = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, D_hidden_1])),    #784x300\n",
        "    'h2': tf.Variable(tf.random_normal([D_hidden_1, D_hidden_2])), #300x300\n",
        "    'out': tf.Variable(tf.random_normal([D_hidden_2, n_classes]))  #300x10\n",
        "}\n",
        "biases_D = {\n",
        "    'b1': tf.Variable(tf.random_normal([D_hidden_1])),             #300x1\n",
        "    'b2': tf.Variable(tf.random_normal([D_hidden_2])),             #300x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model D\n",
        "pred_D = multilayer_perceptron(x, weights_D, biases_D)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 300) b1: (300,)\n",
            "layer_1: (?, 300) W2: (300, 300) b2: (300,)\n",
            "layer_2: (?, 300) W3: (300, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "58GWVJZzrAv3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model D\n",
        "cost_D = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_D, labels=y))\n",
        "\n",
        "# Model D Optimizer\n",
        "optimizer_D = tf.train.AdamOptimizer(learning_rate=learning_rate_D).minimize(cost_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJRLhpzorM2r",
        "colab_type": "code",
        "outputId": "a5a354e0-2d2d-404c-9bc2-e208fc5d3e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Model D\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_D():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_D):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_D, cost_D], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model D Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_D, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model D:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_D()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 53.967102251\n",
            "Epoch: 0010 cost= 1.687477235\n",
            "Epoch: 0019 cost= 0.595191199\n",
            "Epoch: 0028 cost= 0.209805833\n",
            "Epoch: 0037 cost= 0.142061190\n",
            "Epoch: 0046 cost= 0.120157408\n",
            "Epoch: 0055 cost= 0.084393110\n",
            "Epoch: 0064 cost= 0.085849609\n",
            "Epoch: 0073 cost= 0.097922791\n",
            "Epoch: 0082 cost= 0.102317256\n",
            "Epoch: 0091 cost= 0.076751651\n",
            "Epoch: 0100 cost= 0.076111232\n",
            "Model D Optimization Finished!\n",
            "Accuracy of Model D: 0.9697\n",
            "fun: training_loop_D, args: [(), {}] took: 155.47146081924438 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9eitXfNyru8w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model E**\n",
        "\n",
        "*increase nodes*"
      ]
    },
    {
      "metadata": {
        "id": "G6Z_J7Sorze5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model E learning Rate\n",
        "learning_rate_E = 0.001\n",
        "training_epochs_E = 100\n",
        "\n",
        "# Model E Network Parameters\n",
        "E_hidden_1 = 500 # 1st layer number of features\n",
        "E_hidden_2 = 500 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1V9tlkJsBYA",
        "colab_type": "code",
        "outputId": "fcbe01c7-74b7-4b57-9af1-a56fb7615dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model E\n",
        "weights_E = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, E_hidden_1])),    #784x500\n",
        "    'h2': tf.Variable(tf.random_normal([E_hidden_1, E_hidden_2])), #500x500\n",
        "    'out': tf.Variable(tf.random_normal([E_hidden_2, n_classes]))  #500x10\n",
        "}\n",
        "biases_E = {\n",
        "    'b1': tf.Variable(tf.random_normal([E_hidden_1])),             #500x1\n",
        "    'b2': tf.Variable(tf.random_normal([E_hidden_2])),             #500x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model E\n",
        "pred_E = multilayer_perceptron(x, weights_E, biases_E)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 500) b1: (500,)\n",
            "layer_1: (?, 500) W2: (500, 500) b2: (500,)\n",
            "layer_2: (?, 500) W3: (500, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fImVWO8BsXCo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model E\n",
        "cost_E = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_E, labels=y))\n",
        "\n",
        "# Model E Optimizer\n",
        "optimizer_E = tf.train.AdamOptimizer(learning_rate=learning_rate_E).minimize(cost_E)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RMermudOsgOs",
        "colab_type": "code",
        "outputId": "edf88886-9f93-439b-e71c-a43ed25c6170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Model E\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_E():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_E):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_E, cost_E], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model E Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_E, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model E:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_E()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 245.892399372\n",
            "Epoch: 0010 cost= 2.685946415\n",
            "Epoch: 0019 cost= 1.043761824\n",
            "Epoch: 0028 cost= 0.701647883\n",
            "Epoch: 0037 cost= 0.515014493\n",
            "Epoch: 0046 cost= 0.606561599\n",
            "Epoch: 0055 cost= 0.341497255\n",
            "Epoch: 0064 cost= 0.547253780\n",
            "Epoch: 0073 cost= 0.439078821\n",
            "Epoch: 0082 cost= 0.171760189\n",
            "Epoch: 0091 cost= 0.244424880\n",
            "Epoch: 0100 cost= 0.226764511\n",
            "Model E Optimization Finished!\n",
            "Accuracy of Model E: 0.974\n",
            "fun: training_loop_E, args: [(), {}] took: 178.98494935035706 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aBFz6Ot-tFw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model F**\n",
        "\n",
        "*various node volumes for 2 layers*"
      ]
    },
    {
      "metadata": {
        "id": "qX71QsxctOBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model F learning Rate\n",
        "learning_rate_F = 0.001\n",
        "training_epochs_F = 100\n",
        "\n",
        "# Model F Network Parameters\n",
        "F_hidden_1 = 150 # 1st layer number of features\n",
        "F_hidden_2 = 300 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dHAv1RyStZYZ",
        "colab_type": "code",
        "outputId": "6ff5e8dd-48ef-4916-f666-c97c1d1ac721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model F\n",
        "weights_F = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, F_hidden_1])),    #784x150\n",
        "    'h2': tf.Variable(tf.random_normal([F_hidden_1, F_hidden_2])), #150x300\n",
        "    'out': tf.Variable(tf.random_normal([F_hidden_2, n_classes]))  #300x10\n",
        "}\n",
        "biases_F = {\n",
        "    'b1': tf.Variable(tf.random_normal([F_hidden_1])),             #150x1\n",
        "    'b2': tf.Variable(tf.random_normal([F_hidden_2])),             #300x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model F\n",
        "pred_F = multilayer_perceptron(x, weights_F, biases_F)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 150) b1: (150,)\n",
            "layer_1: (?, 150) W2: (150, 300) b2: (300,)\n",
            "layer_2: (?, 300) W3: (300, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i864Coe_twGC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model F\n",
        "cost_F = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_F, labels=y))\n",
        "\n",
        "# Model F Optimizer\n",
        "optimizer_F = tf.train.AdamOptimizer(learning_rate=learning_rate_F).minimize(cost_F)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g8xHoO3nt4vq",
        "colab_type": "code",
        "outputId": "10746e1d-5997-4138-e2ab-35d033f638f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "# Model F\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_F():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_F, cost_F], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model F Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_F, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model F:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_F()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 159.146627426\n",
            "Epoch: 0010 cost= 3.651651075\n",
            "Epoch: 0019 cost= 0.571011755\n",
            "Epoch: 0028 cost= 0.302446496\n",
            "Epoch: 0037 cost= 0.169019668\n",
            "Epoch: 0046 cost= 0.101171108\n",
            "Epoch: 0055 cost= 0.130114581\n",
            "Epoch: 0064 cost= 0.075273835\n",
            "Epoch: 0073 cost= 0.080754856\n",
            "Epoch: 0082 cost= 0.079123368\n",
            "Epoch: 0091 cost= 0.068936129\n",
            "Epoch: 0100 cost= 0.059727455\n",
            "Model F Optimization Finished!\n",
            "Accuracy of Model F: 0.9649\n",
            "fun: training_loop_F, args: [(), {}] took: 148.33707761764526 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GX0x8QP-7qvN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model G **\n",
        "\n",
        "*same nodes as Model F, increase training epochs*"
      ]
    },
    {
      "metadata": {
        "id": "_IBsED0m7qHV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model G learning Rate\n",
        "learning_rate_G = 0.001\n",
        "training_epochs_G = 200\n",
        "\n",
        "# Model G Network Parameters\n",
        "G_hidden_1 = 150 # 1st layer number of features\n",
        "G_hidden_2 = 300 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YptY9sDa7_xb",
        "colab_type": "code",
        "outputId": "b6f8c320-a4d5-4318-c94e-c162a357f4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model G\n",
        "weights_G = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, G_hidden_1])),    #784x150\n",
        "    'h2': tf.Variable(tf.random_normal([G_hidden_1, G_hidden_2])), #150x300\n",
        "    'out': tf.Variable(tf.random_normal([G_hidden_2, n_classes]))  #300x10\n",
        "}\n",
        "biases_G = {\n",
        "    'b1': tf.Variable(tf.random_normal([G_hidden_1])),             #150x1\n",
        "    'b2': tf.Variable(tf.random_normal([G_hidden_2])),             #300x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model G\n",
        "pred_G = multilayer_perceptron(x, weights_G, biases_G)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 150) b1: (150,)\n",
            "layer_1: (?, 150) W2: (150, 300) b2: (300,)\n",
            "layer_2: (?, 300) W3: (300, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HBy-xR1V8NGy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model G\n",
        "cost_G = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_G, labels=y))\n",
        "\n",
        "# Model G Optimizer\n",
        "optimizer_G = tf.train.AdamOptimizer(learning_rate=learning_rate_G).minimize(cost_G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "axlsB_bK8YN8",
        "colab_type": "code",
        "outputId": "1a22ea47-067c-4657-92a0-b81e67aee2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "cell_type": "code",
      "source": [
        "# Model G\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_G():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_G):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_G, cost_G], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model G Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_G, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model G:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_G()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 147.776164714\n",
            "Epoch: 0010 cost= 3.741278299\n",
            "Epoch: 0019 cost= 0.555338832\n",
            "Epoch: 0028 cost= 0.309666292\n",
            "Epoch: 0037 cost= 0.130260477\n",
            "Epoch: 0046 cost= 0.110919527\n",
            "Epoch: 0055 cost= 0.111610215\n",
            "Epoch: 0064 cost= 0.088813144\n",
            "Epoch: 0073 cost= 0.131664972\n",
            "Epoch: 0082 cost= 0.089893032\n",
            "Epoch: 0091 cost= 0.079846640\n",
            "Epoch: 0100 cost= 0.082471903\n",
            "Epoch: 0109 cost= 0.074195219\n",
            "Epoch: 0118 cost= 0.070600756\n",
            "Epoch: 0127 cost= 0.058322106\n",
            "Epoch: 0136 cost= 0.050874336\n",
            "Epoch: 0145 cost= 0.054817912\n",
            "Epoch: 0154 cost= 0.048086062\n",
            "Epoch: 0163 cost= 0.080321500\n",
            "Epoch: 0172 cost= 0.058127463\n",
            "Epoch: 0181 cost= 0.029928784\n",
            "Epoch: 0190 cost= 0.033959699\n",
            "Epoch: 0199 cost= 0.057468805\n",
            "Model G Optimization Finished!\n",
            "Accuracy of Model G: 0.9657\n",
            "fun: training_loop_G, args: [(), {}] took: 297.589617729187 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LUCLfp5g849z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Model H**\n",
        "\n",
        "*same nodes as Model F and G, less training epochs*"
      ]
    },
    {
      "metadata": {
        "id": "XtG8qL6y9Imc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model H learning Rate\n",
        "learning_rate_H = 0.001\n",
        "training_epochs_H = 50\n",
        "\n",
        "# Model H Network Parameters\n",
        "H_hidden_1 = 150 # 1st layer number of features\n",
        "H_hidden_2 = 300 # 2nd layer number of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hm0OQbFh9JeO",
        "colab_type": "code",
        "outputId": "69798adf-5226-4a5f-d8e1-25d535ea5ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "# Store layers weight & bias for Model H\n",
        "weights_H = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, H_hidden_1])),    #784x150\n",
        "    'h2': tf.Variable(tf.random_normal([H_hidden_1, H_hidden_2])), #150x300\n",
        "    'out': tf.Variable(tf.random_normal([H_hidden_2, n_classes]))  #300x10\n",
        "}\n",
        "biases_H = {\n",
        "    'b1': tf.Variable(tf.random_normal([H_hidden_1])),             #150x1\n",
        "    'b2': tf.Variable(tf.random_normal([H_hidden_2])),             #300x1\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))              #10x1\n",
        "}\n",
        "\n",
        "# Construct Model H\n",
        "pred_H = multilayer_perceptron(x, weights_H, biases_H)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (?, 784) W1: (784, 150) b1: (150,)\n",
            "layer_1: (?, 150) W2: (150, 300) b2: (300,)\n",
            "layer_2: (?, 300) W3: (300, 10) b3: (10,)\n",
            "out_layer: (?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nrmGd8Mj9Nxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Cross entropy loss function for Model H\n",
        "cost_H = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_H, labels=y))\n",
        "\n",
        "# Model H Optimizer\n",
        "optimizer_H = tf.train.AdamOptimizer(learning_rate=learning_rate_H).minimize(cost_H)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZG8C877H9Qdz",
        "colab_type": "code",
        "outputId": "e4a0baaf-00fa-4fdd-c4f4-78e4a1d5940d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "# Model H\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "@timing\n",
        "def training_loop_H():\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs_H):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer_H, cost_H], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "    print(\"Model G Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred_H, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    # To keep sizes compatible with model\n",
        "    print (\"Accuracy of Model G:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
        "    \n",
        "training_loop_H()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 157.657458832\n",
            "Epoch: 0010 cost= 3.629901862\n",
            "Epoch: 0019 cost= 0.562861310\n",
            "Epoch: 0028 cost= 0.272639280\n",
            "Epoch: 0037 cost= 0.166516822\n",
            "Epoch: 0046 cost= 0.172898383\n",
            "Model G Optimization Finished!\n",
            "Accuracy of Model G: 0.9579\n",
            "fun: training_loop_H, args: [(), {}] took: 74.94109678268433 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zbqJNpMSOQav",
        "colab_type": "code",
        "outputId": "7a532e09-a0cf-42a0-c94e-c9c66d653eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "Models = ['A','B','C','D','E','F','G','H']\n",
        "Layers = [2,2,2,2,2,2,2,2]\n",
        "LR = [.001, .1, .001, .01, .001, .001, .001, .001]\n",
        "Layer1_Nodes = [28,28,300,300,500,150,150,150]\n",
        "Layer2_Nodes = [28,28,300,300,500,300,300,300]\n",
        "Execution = [136.7, 137.7, 155.4, 155.5, 178.9, 148.3, 297.6, 74.9]\n",
        "_Accuracy = ['95%','21%','97%','97%','97%','96%','97%','96%']\n",
        "Loss = [0.065, 1.83, 0.103, 0.076, 0.220, 0.059, 0.057, 0.17]\n",
        "Epochs = [100,100,100,100,100,100,200,50]\n",
        "\n",
        "table = pd.DataFrame(data={\"Learning Rate\": LR, \"NN Layers\": Layers, \"Layer 1 Nodes\": Layer1_Nodes, \"Layer 2 Nodes\": Layer2_Nodes, \"Execution Time\": Execution, \"Epochs\": Epochs, \"Accuracy\": _Accuracy, \"Loss\": Loss}, index=Models)\n",
        "table"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Epochs</th>\n",
              "      <th>Execution Time</th>\n",
              "      <th>Layer 1 Nodes</th>\n",
              "      <th>Layer 2 Nodes</th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Loss</th>\n",
              "      <th>NN Layers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>95%</td>\n",
              "      <td>100</td>\n",
              "      <td>136.7</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.065</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>21%</td>\n",
              "      <td>100</td>\n",
              "      <td>137.7</td>\n",
              "      <td>28</td>\n",
              "      <td>28</td>\n",
              "      <td>0.100</td>\n",
              "      <td>1.830</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>97%</td>\n",
              "      <td>100</td>\n",
              "      <td>155.4</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.103</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>D</th>\n",
              "      <td>97%</td>\n",
              "      <td>100</td>\n",
              "      <td>155.5</td>\n",
              "      <td>300</td>\n",
              "      <td>300</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.076</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>E</th>\n",
              "      <td>97%</td>\n",
              "      <td>100</td>\n",
              "      <td>178.9</td>\n",
              "      <td>500</td>\n",
              "      <td>500</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.220</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>F</th>\n",
              "      <td>96%</td>\n",
              "      <td>100</td>\n",
              "      <td>148.3</td>\n",
              "      <td>150</td>\n",
              "      <td>300</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.059</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>G</th>\n",
              "      <td>97%</td>\n",
              "      <td>200</td>\n",
              "      <td>297.6</td>\n",
              "      <td>150</td>\n",
              "      <td>300</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.057</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>H</th>\n",
              "      <td>96%</td>\n",
              "      <td>50</td>\n",
              "      <td>74.9</td>\n",
              "      <td>150</td>\n",
              "      <td>300</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.170</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Accuracy  Epochs  Execution Time  Layer 1 Nodes  Layer 2 Nodes  \\\n",
              "A      95%     100           136.7             28             28   \n",
              "B      21%     100           137.7             28             28   \n",
              "C      97%     100           155.4            300            300   \n",
              "D      97%     100           155.5            300            300   \n",
              "E      97%     100           178.9            500            500   \n",
              "F      96%     100           148.3            150            300   \n",
              "G      97%     200           297.6            150            300   \n",
              "H      96%      50            74.9            150            300   \n",
              "\n",
              "   Learning Rate   Loss  NN Layers  \n",
              "A          0.001  0.065          2  \n",
              "B          0.100  1.830          2  \n",
              "C          0.001  0.103          2  \n",
              "D          0.010  0.076          2  \n",
              "E          0.001  0.220          2  \n",
              "F          0.001  0.059          2  \n",
              "G          0.001  0.057          2  \n",
              "H          0.001  0.170          2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "eRDNm79DZPVq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **Recommendation**\n",
        "\n",
        "*Learning rate*: Learning rate was adjusted through models A – D. The pairs of Models A-B and C-D each had the same node design and similar execution time. Within the two Model pairs, the Model with the smallest learning rate recorded the smallest loss.  As Models A and B had much smaller nodes, the learning rate’s impact to the model accuracy seemed to be greater. \n",
        "\n",
        "*Execution time:* The execution time seems to be most impacted by total nodes and of course the total training epochs. Models with similar nodes but different learning rate did not see much variance in execution time. The biggest variance in execution time can be observed between Models F – H, where the nodes and learning rate remained the same, but the training epochs changed.\n",
        "\n",
        "*Loss*: Observing the loss of each model, Models E and B stick out. Model B with the largest loss value, had both a low number of layer nodes and a low learning rate. Model E, however, had the highest nodes and a low learning rate. This suggests there may be diminishing returns for node volume within the layers. \n",
        "\n",
        "\n",
        "Of the observed Models, **Model H is the most efficient and recommended model**. It achieves above average model accuracy and it is, by far, executed the quickest. \n",
        "\n"
      ]
    }
  ]
}